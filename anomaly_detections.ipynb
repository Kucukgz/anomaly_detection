{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Image Processing Library.\n",
    "\n",
    "Wow!\n",
    "\"\"\"\n",
    "import os\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "from skimage.restoration import estimate_sigma\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.ndimage import laplace\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.cluster import KMeans\n",
    "from pyoptflow import HornSchunck\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from numpy import quantile, where, random\n",
    "import numpy\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _get_measurements(folder):\n",
    "    # list image files\n",
    "    filenames = os.listdir(folder)\n",
    "    # sort the image filenames\n",
    "    filenames = sorted(filenames, key=lambda v: v.upper())\n",
    "    nl, bs, bs2, ai, dl, di, cl, mz = [], [], [], [], [], [], [], []\n",
    "    for filename in filenames:\n",
    "        print(filename)\n",
    "        filename = os.path.join(folder, filename)\n",
    "        im = imread(filename)\n",
    "        im = np.moveaxis(im, 0, -1)\n",
    "        for i in range(im.shape[2]):\n",
    "            nl.append(estimate_sigma(\n",
    "                im[:, :, i], multichannel=False, average_sigmas=True))\n",
    "            imlap = laplace(im[:, :, i])\n",
    "            bs.append(imlap.var())  # Blurriness Score\n",
    "            im2 = gaussian_filter(im[:, :, i], sigma=3)\n",
    "            bs2.append(im2.var())  # Blurriness Score with Gaussian Filter\n",
    "            ai.append(im[:, :, i].mean())  # Average Intensity\n",
    "            dl.append(_get_dark_light(im[:, :, i]))  # Darkness Level\n",
    "            di.append(_get_dominant_intensity(\n",
    "                im[:, :, i]))  # Dominant intensity\n",
    "            imgx, imgy = np.gradient(im[:, :, i])\n",
    "            img = np.sqrt(np.power(imgx, 2) + np.power(imgy, 2))\n",
    "            # Contrast Level\n",
    "            cl.append(np.sum(img) / (im.shape[0] * im.shape[1]))\n",
    "        for i in range(im.shape[2] - 1):\n",
    "            _, _, m, _ = _motion_estimation(im[:, :, i], im[:, :, i + 1])\n",
    "            ali = np.sum(m)\n",
    "            mz.append(ali)  # Motion Estimation\n",
    "    return nl, bs, bs2, ai, dl, di, cl, mz\n",
    "\n",
    "\n",
    "def _motion_estimation(im1, im2, a=1.0, n=100):\n",
    "    u, v = HornSchunck(im1, im2, alpha=a, Niter=n)\n",
    "    m = np.sqrt(np.power(u, 2) + np.power(v, 2))  # magnitude\n",
    "    a = np.arctan2(u, v)  # angle\n",
    "    return m, v, u, a\n",
    "\n",
    "\n",
    "def _get_dark_light(im):\n",
    "    # im - grayscale [0-255]\n",
    "    # intensity palette of the image\n",
    "    palette = defaultdict(int)\n",
    "    for pixel in np.nditer(im):\n",
    "        palette[int(pixel)] += 1\n",
    "    # sort the intensity present in the image\n",
    "    sorted_x = sorted(palette.items(), key=itemgetter(1), reverse=True)\n",
    "    light_shade, dark_shade, shade_count, pixel_limit = 0, 0, 0, 25\n",
    "    for _, x in enumerate(sorted_x[:pixel_limit]):\n",
    "        if x[0] <= 20:  # dull : too much darkness\n",
    "            dark_shade += x[1]\n",
    "        if x[0] >= 240:  # bright : too much whiteness\n",
    "            light_shade += x[1]\n",
    "        shade_count += x[1]\n",
    "    light_percent = round((float(light_shade) / shade_count) * 100, 2)\n",
    "    dark_percent = round((float(dark_shade) / shade_count) * 100, 2)\n",
    "    return dark_percent\n",
    "\n",
    "\n",
    "def _get_dominant_intensity(im):\n",
    "    # k-means\n",
    "    kmeans_cluster = KMeans(n_clusters=5)\n",
    "    kmeans_cluster.fit(im)\n",
    "    cluster_centers = kmeans_cluster.cluster_centers_\n",
    "    cluster_labels = kmeans_cluster.labels_\n",
    "    # dominant intensity\n",
    "    palette = np.uint8(cluster_centers)\n",
    "    dominant_intensity = palette[np.argmax(\n",
    "        np.unique(cluster_labels, return_counts=True)[1])]\n",
    "    # from vector [1,...,z] - > 1 number\n",
    "    dominant_intensity = np.median(dominant_intensity)\n",
    "    return dominant_intensity\n",
    "\n",
    "\n",
    "def figure(x):\n",
    "    \"\"\"To figure all features.\"\"\"\n",
    "    for i, j in x:\n",
    "        plt.figure()\n",
    "        plt.scatter(range(len(i)), np.sort(i))\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel(f'{j}')\n",
    "        plt.title(f\"{j} Distribution\")\n",
    "        sns.despine()\n",
    "        plt.show()\n",
    "        plt.figure()\n",
    "        sns.distplot(i)\n",
    "        plt.title(f\"Distribution of {j}\")\n",
    "        sns.despine()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def anomaly_score(x):\n",
    "    \"\"\"To figure out anomaly scores.\"\"\"\n",
    "    # must calibrate it for all measurements\n",
    "    outliers = []\n",
    "    for i, j in x:\n",
    "        pd_i = pd.DataFrame(i)\n",
    "        isolation_forest = IsolationForest(\n",
    "            n_estimators=100, contamination=float(.00001))\n",
    "        isolation_forest.fit(pd_i.values.reshape(-1, 1))\n",
    "        xx = np.linspace(pd_i.min(), pd_i.max(), len(pd_i)).reshape(-1, 1)\n",
    "        anomaly_score = isolation_forest.decision_function(xx)\n",
    "        outlier = isolation_forest.predict(xx)\n",
    "        isoF_outliers_values = pd_i[isolation_forest.predict(xx) == -1]\n",
    "        outliers.append(isoF_outliers_values)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(xx, anomaly_score, label='anomaly score')\n",
    "        plt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score),\n",
    "                         where=outlier == -1, color='r',\n",
    "                         alpha=.4, label='Outlier Region')\n",
    "        plt.legend()\n",
    "        plt.title(f'Isolation Forest Detection of {j}', fontsize=15)\n",
    "        plt.ylabel(f'Anomaly Score', fontsize=15)\n",
    "        plt.savefig(f'isoF_images/LOF_{j}', format='png', dpi=1200)\n",
    "        plt.show()\n",
    "    return outliers\n",
    "\n",
    "\n",
    "def svm_anomaly_score(df_data):\n",
    "    \"\"\"To figure out anomaly scores.\"\"\"\n",
    "    # must calibrate it for all measurements\n",
    "    outliers = []\n",
    "    for label, content in df_data.items():\n",
    "        df_data[f'{label}'] = df_data[f'{label}'].fillna(0)\n",
    "        svm = OneClassSVM(kernel='rbf', gamma=0.00001, nu=0.03)\n",
    "        pred = svm.fit_predict(df_data[f'{label}'].values.reshape(-1, 1))\n",
    "        scores = svm.score_samples(df_data[f'{label}'].values.reshape(-1, 1))\n",
    "\n",
    "        thresh = quantile(scores, 0.008)\n",
    "        feature_score = []\n",
    "        anom = []\n",
    "        inliers_feature_score = []\n",
    "        inliers = []\n",
    "        kazim = []\n",
    "        ali = []\n",
    "        for i, j in enumerate(scores):\n",
    "            if j <= thresh:\n",
    "                outliers.append(i)\n",
    "                anom.append(j)\n",
    "                feature_score.append(df_data[f'{label}'][i])\n",
    "                ali.append(i)\n",
    "            else:\n",
    "                inliers.append(j)\n",
    "                inliers_feature_score.append(df_data[f'{label}'][i])\n",
    "                kazim.append(i)\n",
    "\n",
    "        inliers_pd = pd.DataFrame({'inliers': inliers, 'inliers_feature_score': inliers_feature_score,\n",
    "                                  'inliers_index': kazim})\n",
    "        pd_anom = pd.DataFrame({'AnomScore': anom, 'FeatureScore': feature_score, 'outlier_index': ali})\n",
    "\n",
    "        fig = go.Figure()\n",
    "        fig.update_layout(title={\n",
    "            'text': f\"SVM Detection of {label}\", 'y': 0.97, 'x': 0.5},\n",
    "            paper_bgcolor='white', plot_bgcolor=\"rgb(211, 216, 230)\",\n",
    "            # xaxis_title=\" \",\n",
    "            yaxis_title=\"Anomaly Score\",\n",
    "            font=dict(family=\"Courier New, monospace\", size=50, color=\"rgb(10, 16, 87)\"),\n",
    "            title_font_color='rgb(145, 0, 0)',\n",
    "            shapes=[dict(\n",
    "                    type=\"line\",\n",
    "                    xref=\"x\",\n",
    "                    yref=\"y\",\n",
    "                    x0=df_data[f'{label}'].min(),\n",
    "                    y0=thresh,\n",
    "                    x1=df_data[f'{label}'].max(),\n",
    "                    y1=thresh,\n",
    "                    opacity=1,\n",
    "                    line=dict(color='blue', dash='dot')\n",
    "                    )])\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=inliers_pd['inliers_feature_score'], y=inliers_pd['inliers'],\n",
    "                                 mode='markers', marker=dict(size=6, color='rgb(0, 0, 0)'),\n",
    "                                 name='Normal', marker_symbol='circle'))\n",
    "        fig.add_trace(go.Scatter(x=pd_anom['FeatureScore'], y=pd_anom['AnomScore'],\n",
    "                                 mode='markers', marker=dict(size=14, color='rgb(255, 0, 0)'),\n",
    "                                 name='Abnormal', marker_symbol=206))\n",
    "\n",
    "        fig.show()\n",
    "        plotly.io.write_image(fig, f'SVM_images/{label}.png', width=2560, height=1440)\n",
    "\n",
    "    return outliers\n",
    "\n",
    "\n",
    "def LOF_anomaly_score2(x):\n",
    "    \"\"\"To figure out anomaly scores.\"\"\"\n",
    "    # must calibrate it for all measurements\n",
    "    outliers = []\n",
    "    outliers_list = []\n",
    "    for i, j in x:\n",
    "        pd_i = pd.DataFrame(i)\n",
    "        method = 1\n",
    "        k = 30\n",
    "        clf = LocalOutlierFactor(n_neighbors=k , algorithm='auto', contamination=0.1, n_jobs=-1)\n",
    "        clf.fit(pd_i)\n",
    "        # Record k neighborhood distance\n",
    "        pd_i['k distances'] = clf.kneighbors(pd_i)[0].max(axis=1)\n",
    "        # Record LOF factor，take negative\n",
    "        pd_i['local outlier factor'] = -clf._decision_function(pd_i.iloc[:, :-1])\n",
    "        # Separate group points and normal points according to the threshold\n",
    "        outliers = pd_i[pd_i['local outlier factor'] > method].sort_values(by='local outlier factor')\n",
    "        inliers = pd_i[pd_i['local outlier factor'] <= method].sort_values(by='local outlier factor')\n",
    "        # Figure\n",
    "        plt.rcParams['axes.unicode_minus'] = False  # display the negative sign\n",
    "        plt.figure(figsize=(8, 4)).add_subplot(111)\n",
    "        plt.scatter(pd_i[pd_i['local outlier factor'] > method].index,\n",
    "                    pd_i[pd_i['local outlier factor'] > method]['local outlier factor'], c='red', s=50,\n",
    "                    marker='.', alpha=None,\n",
    "                    label='outliers')\n",
    "        plt.scatter(pd_i[pd_i['local outlier factor'] <= method].index,\n",
    "                    pd_i[pd_i['local outlier factor'] <= method]['local outlier factor'], c='black', s=50,\n",
    "                    marker='.', alpha=None, label='inliers')\n",
    "        plt.hlines(method, -2, 2 + max(pd_i.index), linestyles='--')\n",
    "        plt.xlim(-2, 2 + max(pd_i.index))\n",
    "        plt.title(f'LOF Local outlier detection of {j}', fontsize=13)\n",
    "        plt.ylabel('Anamoly Score', fontsize=15)  # Local outlier Factors\n",
    "        plt.legend()\n",
    "        plt.savefig(f'LOF_images/LOF_{j}', format='png', dpi=1200)\n",
    "        plt.show()\n",
    "        outliers_list.append(list(outliers.index))\n",
    "    return outliers_list\n",
    "\n",
    "\n",
    "def skew_kurt(x):\n",
    "    \"\"\"To figure out distribution.\"\"\"\n",
    "    for i, j in x:\n",
    "        pd_x = pd.DataFrame(i)\n",
    "        print(f\"Skewness of {j}: %f\" % pd_x.skew())\n",
    "        print(f\"Kurtosis of {j}: %f\" % pd_x.kurt())\n",
    "\n",
    "\n",
    "def grabbing_outliers(x, folder):\n",
    "    \"\"\"To grab outliers.\"\"\"\n",
    "    outliers_list = []\n",
    "    for i in x:\n",
    "        detections = i.index.values.tolist()\n",
    "        outliers_list.append(detections)\n",
    "    return outliers_list\n",
    "\n",
    "\n",
    "def dropping_outliers(x):\n",
    "    \"\"\"To drop off outliers.\"\"\"\n",
    "    dropped_outliers = []\n",
    "    for i in x:\n",
    "        # print(i)\n",
    "        for j in i:\n",
    "            # print(j)\n",
    "            dropped_outliers.append(j)\n",
    "    return dropped_outliers\n",
    "\n",
    "\n",
    "def save_values(values, path):\n",
    "    \"\"\"To save numpy arrays.\"\"\"\n",
    "    k = 0\n",
    "    for i, j in values:\n",
    "        npy_data = np.array(i)\n",
    "        target_name = os.path.join(path, j + \"_\" + str(k) + \".npy\")\n",
    "        np.save(target_name, npy_data)\n",
    "        k += 1\n",
    "    return\n",
    "\n",
    "\n",
    "def load_values(path):\n",
    "    \"\"\"To load numpy arrays.\"\"\"\n",
    "    names = {i: [] for i in os.listdir(path)}\n",
    "    filename = []\n",
    "    np_list = []\n",
    "    np_data = ()\n",
    "    df = ()\n",
    "    name_list = []\n",
    "    for key, value in names.items():\n",
    "        filename.append(key)\n",
    "        np_list.append(value)\n",
    "    for i in filename:\n",
    "        splitting_name = i.replace(\".npy\", \"\")\n",
    "        # number = int(splitting_name.split('_')[1])\n",
    "        name = splitting_name.split('_')[0]\n",
    "        np_list = np.load(path + i)\n",
    "        np_data += ((np_list.tolist(), name),)\n",
    "        df += (np_list.tolist(),)\n",
    "        name_list.append(name)\n",
    "    df_data = pd.DataFrame(df)\n",
    "    df_data = df_data.T\n",
    "    df_data.columns = name_list\n",
    "    return np_data, df_data\n",
    "\n",
    "\n",
    "def save_txt(x, name):\n",
    "    \"\"\"To save anomalyies as a text file.\"\"\"\n",
    "    with open(f\"{name}_anomalies.txt\", \"w\") as f:\n",
    "        for s in x:\n",
    "            f.write(str(s) + \"\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    # load images\n",
    "    folder = '/home/burak/vsc/first_part/__private__/__private__ (copy)/Data/original'\n",
    "    path = \"measurements/single_npy/\"\n",
    "\n",
    "    # Save measurements as numpy arrays\n",
    "    # nl, bs, bs2, ai, dl, di, cl, mz = _get_measurements(folder)\n",
    "    # values = ((nl, 'NoiseLevel'), (bs, 'BlurrinessScore'), (bs2, 'BlurrinessScoreWithGaussianFilter'),\n",
    "    #           (ai, 'AverageIntensity'), (dl, 'DarknessLevel'), (di, 'DominantIntensity'), (cl, 'ContrastLevel'),\n",
    "    #           (mz, 'MotionEstimation'))\n",
    "    # save_values(values, path)\n",
    "\n",
    "    values, df_data = load_values(path)\n",
    "    skew_kurt(values)\n",
    "    figure(values)\n",
    "\n",
    "    isoF_outliers = anomaly_score(values)\n",
    "    grapped_outliers = grabbing_outliers(isoF_outliers, folder)\n",
    "    isoF_dropped_outliers = set(dropping_outliers(grapped_outliers))\n",
    "\n",
    "    svm_outliers = svm_anomaly_score(df_data)\n",
    "    svm_dropped_outliers = set(svm_outliers)\n",
    "\n",
    "    lof_outliers = LOF_anomaly_score2(values)\n",
    "    grapped_outliers = [val for sublist in lof_outliers for val in sublist]\n",
    "    lof_dropped_outliers = set(grapped_outliers)\n",
    "\n",
    "    save_txt(sorted(isoF_dropped_outliers), name='isoF')\n",
    "    save_txt(sorted(svm_dropped_outliers), name='svm')\n",
    "    save_txt(sorted(lof_dropped_outliers), name='LOF')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bitb1003144471e4b308bb505b884110c36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}